---
title: 'STAT 428: Homework 4: <br> Chapter6 Monte Carlo Methods in Inference'
author: "Fu, Xingyu, xingyuf2 <br>"
date: "Due Week 9 Saturday, October 27 by 11.59 PM on Compass"
output:
  html_document:
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
```
---------------------------------------

Please refer to the [**detailed homework policy document**](https://compass2g.illinois.edu/bbcswebdav/pid-3348982-dt-content-rid-35374883_1/courses/stat_428_120188_166591/1_Info/Syllabus/homework_policy.html) on Course Page for information about homework formatting, submission, and grading. Additional policies:

1. Whenever you are asked about an algorithm or a code implementing such, you have to either explain the algorithm clearly in steps or provide detailed documentation/comments on your R code explaining what you are doing. This is to ensure we fully understand what your code is trying to achieve since it can be hard to depict codes sometimes.

2. When using an R code chunk in the RmD with {r}, please use {r,eval=TRUE, echo=TRUE, include=TRUE} instead so that your chunk and results show in the html. This is just to help us grade faster.

3. When using external packages, please mention clearly what R packages you are using so that we can install those when running your codes. Furthermore, please call the package in your code using the "library" function so that your code shows no error when running.

---------------------------------------

## Exercise 1
**Antithetic method.**
Suppose we want to estimate the mean of the Weibull distribution with the following pdf
$$
f(x) = \frac{4}{5} \left(\frac{x}{5}\right)^3 e^{-(x/5)^4}, \quad
0<x<\infty.
$$

1. Use the inverse CDF method to estimate the mean. (Sample size $n=1000$)
$$F(t) = 1 - e^{-x^4/5^4}, t>0. \quad \quad F(t) = 0, t \leq 0.$$
So $$F^{-1}(u) = (-5^4 ln(1-u))^{1/4}$$
```{r,eval=TRUE, echo=TRUE, include=TRUE}
u <- runif(1000)             #Generating standard uniform variates
X <- (log(1-u)*(-5^4))^(1/4)       #Generating the random values from distribution f using inverse transform method
(est1 = mean(X))
(var1 = var(X))
```

2. Use an antithetic method to estimate the mean. (Sample size $n=1000$)
mean = $\int_0^{\infty} {\frac{4x^3}{5^4}e^{-\frac{x^4}{5^4}}x} dx$
```{r,eval=TRUE, echo=TRUE, include=TRUE}
u <- runif(1000)
X = ((log(1-u)*(- 5^4))^(1/4) + (log(u)*(- 5^4))^(1/4)) / 2
(est2 = mean(X))
(var2 = var(X))
```

3. Compare the variance of these two estimates. Which one has smaller variance?

<br>As we can see above, var1 > var2, the second one has smaller variance.

## Exercise 2
**Bayesian Statistics**
Suppose $X_1,\ldots,X_n$ are $n$ independent and identical distributed random variables from $Exp(\theta)$, where $\theta$ is the unknown parameter. So, $$f(x|\theta) = \theta \; e^{-\theta x}, \quad x \ge 0.$$

We assume the prior distribution on $\theta$ is Gamma distribution $Gamma(3,2)$. $$ g(\theta) = 4 x^2 e^{-2x}, \quad x \ge 0. $$

1. Write down the posterior distribution of $\theta$, $g(\theta|X)$.
$$g(\theta|X) = g(\theta|X_1, X_2, ..., X_n) 
\propto (\prod_i^n\theta e^{-\theta X_i}) \cdot (4 \theta ^2e^{-2\theta})
\propto \theta^{n+2} \cdot e^{-\theta \sum_i^nX_i - 2\theta}$$

2. Suppose $n=5$ and we observe that $x_1=0.5, x_2=1, x_3=0.1, x_4=1.7, x_5=1.4$. Please estimate the posterior mean of $\theta$ based on $1000$ simulated $\theta$ from its prior distribution.
```{r,eval=TRUE, echo=TRUE, include=TRUE}
x <- c(0.5, 1, 0.1, 1.7, 1.4) #observed data
x_sum = sum(x)

m <- 1000
theta <- rgamma(m, 3, 2)
h <- theta^5 * exp(-theta * x_sum) # compute h(theta)
C <- mean(h) # estimate the normalizing constant
posterior_mean <- mean(theta * h) / C # estimate the posterior mean
posterior_mean
```

3. Suppose $n=5$ and we observe that $x_1=0.5, x_2=1, x_3=0.1, x_4=1.7, x_5=1.4$.

    a. Design an acceptance-rejection sampling algorithm to generate $1000$ (accepted) samples of $\theta$ from the posterior distribution of $\theta$. Write down your algorithm with your instrumental distribution $g(\theta)$. (Hint: for acceptance-rejection sampling method, the normalizing constant in the posterior distribution can be ignored.)
    
    The target desity $f$ is $$f(\theta)= \theta^{n+2} \cdot e^{-\theta \sum_i^nX_i - 2\theta} = \theta^{n+2} \cdot e^{-\theta n \bar{x} - 2\theta}$$
    Let the instrumental distribution $g$ be $$g(\theta) =n\bar{x}e^{-n\overline{x}\theta} = Exp(n\bar{x})$$
    Suppose $f(\theta) / g(\theta) \leq C$, below we calculate $C$ in R.
    
    So our method is: 
    1. Generate $Y \sim g$, generate $U \sim U[0,1]$.  
    2. Accept Y as a sample of f if $U \leq \frac{f(Y)}{C g(Y)}$  
    3. Until we've generated enough samples, go back to Step 1.  
    
    b. Implement your acceptance-rejection sampling algorithm with R code. Plot the histogram of your generated sample and compare your sample mean with your estimated posterior mean obatained in Ex.2.2.
    ```{r,eval=TRUE, echo=TRUE, include=TRUE}
    x <- c(0.5, 1, 0.1, 1.7, 1.4) #observed data
    n <- length(x)
    x_bar <- mean(x)
    f_g <- function(theta) { # f / g
      f <- theta^(n + 2) * exp(-n * theta * x_bar - 2 * theta) 
      g <- n * x_bar * exp(- n * x_bar * theta)
      return(f / g)
    }
    C <- optimize(f = f_g, c(0, 200), maximum = T)$objective # find the upper bound of f / g
    C
    samples <- numeric(1000)
    acc_cnt = 0
    while (acc_cnt < 1000) {
      u <- runif(1)
      y <- rexp(1, rate = n * x_bar)
      f <- y^(n + 2) * exp( - n * y * x_bar - 2 * y)
      g <- n * x_bar * exp(-n * x_bar * y)
      if (u <= f/g/C) {
        acc_cnt <- acc_cnt + 1
        samples[acc_cnt] <- y
      }
    }
    mean(samples)
    hist(samples, breaks=20, probability = TRUE, main="The sample histogram")
    ```
    
    <br>I think my sample mean is close to my estimated posterior mean obatained in Ex.2.2.

## Exercise 3
**Exercise 6.6**

Do Problem 6.6 from the book. You can follow these steps if the question is confusing to you.

1) First you need to figure out how to find skewness of a sample. (Refer to Example 6.8, look at the R code chunk with the function `sk`, that gives you skewness of any data.)

2) Next you need to simulate a bunch of data and find skewness for several groups of them. The pseudo code is

```{r, eval=FALSE, echo=TRUE}
for(i in 1:1000){
  x <- 1000 standard random normal variables
  b[i] = sk(x)    #Here sk is the function in the book example
}
```

Now you end up with vector $b=(b_1,b_2,.,b_{1000})$. Consider this vector `b` to be your sample data and forget everything else. The rest is simple Monte Carlo.

3) Find quantiles of your skewness vector "b".  __Hint__ : The R function "quantile" helps. The "probs" parameter of your "quantile" function is the vector (0.025, ..., 0.975). These are your estimates of quantiles.

4) Now the question asks you to get variance. This formula is given in Chapter 2.6 Equation 2.14. In that formula, q is the quantile you want to find the variance of. If you are working with 2.5% quantile, then q = 0.025, n = your sample size (in my code 1000), f(x_q) = dnorm(your quantile estimate). Square root of this formula gives you standard error.

5) Last part asks you to compare your quantile estimates (in (3)) with the theoretical values. Theoretical distribution is given in the question, it is `N(0, 6/n)`, n being your sample size. ___Hint__: Do `qnorm(0.025, 0, sqrt(6/1000))` to get theoretical estimate and compare this value with the first element of quantile estimate. Similarly, do it for the rest of the values.

```{r}
quantiles <- c(0.025, 0.05, 0.95, 0.975)

sk <- function(x) {
  #computes the sample skewness coeff.
  xbar <- mean(x)
  m3 <- mean((x - xbar)^3)
  m2 <- mean((x - xbar)^2)
  return(m3 / m2^1.5)
}

m <- 1000
b <- numeric(m) # a vector of sample sizes
for(i in 1 : m) {
  x <- rnorm(1000, 0, 1)
  b[i] <- sk(x)
}
ests <- quantile(b, quantiles)

se <- function(q, n, ests) {
  return (q * (1 - q) / n / dnorm(ests)^2)
}
est_se <- numeric(length(quantiles))
for (i in 1 : length(est_se)) {
  est_se[i] <- sqrt(se(quantiles[i], m, ests[i]))
}

real <- qnorm(quantiles, 0, sqrt(6/1000))
table <- cbind(quantiles, ests, real, est_se)
colnames(table) <- c("quantile", "estimated quantile", "theoretical quantile", "estimate's standard error")
table
```

## Exercise 4-8

Do the following problems from the book: 6.1, 6.2, 6.3, 6.5, 6.8.

### 6.1 
```{r,eval=TRUE, echo=TRUE, include=TRUE}
m <- 2000
K <- 9
n <- 20
tmean <- matrix(0,m,K)
mse_est <- numeric(K)
mse_se <- numeric(K)
for(k in 1:K){
  for (i in 1:m) {
    x <- sort(rcauchy(n))
    tmean[i,k] <- mean(x[(k+1):(n-k)])
    }
  mse_est[k] <- mean(tmean[,k]^2)
  mse_se[k] <- sqrt(sum((tmean[,k] - mean(tmean[,k]))^2)) / m
}
table <- cbind(seq(1:K),round(mse_est,5),round(mse_se,5))
colnames(table) <- c("k", "Estimated MSE of level k trimmed means", "Standard Error")
knitr::kable(table, caption = 'Estimates of MSE')
```

### 6.2
```{r,eval=TRUE, echo=TRUE, include=TRUE}
alpha = 0.05
mu <- c(seq(350, 650, 10)) #alternative H
n <- 20
sigma <- 100
m <- 1000
mu0 <- 500
M <- length(mu)
power <- numeric(M)
for (i in 1 : M) {
  pvalues <- replicate(m, expr = {x <- rnorm(n, mean = mu[i], sd = sigma)
      ttest <- t.test(x, alternative = "two.sided", mu = mu0)
      ttest$p.value})
  power[i] <- mean(pvalues <= alpha)
}

library(Hmisc) #for errbar
plot(mu, power)
abline(v = mu0, lty = 1)
abline(h = alpha, lty = 1)
se <- sqrt(power * (1-power) / m)
errbar(mu, power, yplus = power+se, yminus = power-se,xlab = bquote(theta))
lines(mu, power, lty=3)
detach(package:Hmisc)
```

### 6.3
```{r,eval=TRUE, echo=TRUE, include=TRUE}
n <- seq(10,50,10) #sample size
mu <- c(seq(350, 650, 10))
m <- 1000
M <- length(mu)
N <- length(n)
power <- matrix(0,M,N)
for(j in 1:N){
  for (i in 1:M){
    mu1 <- mu[i]
    pvalues <- replicate(m, expr = {
      #simulate under alternative mu1
      x <- rnorm(n[j], mean = mu1, sd = 100)
      ttest <- t.test(x,
      alternative = "two.sided", mu = 500)
      ttest$p.value })
    power[i, j] <- mean(pvalues <= .05)
  }
}
plot(mu, power[,1], type="l", lty = 2, col = 1, main="Power curve", xlab="mu", ylab="Power")
lines(mu, power[,2], lty = 3, col = 2)
lines(mu, power[,3], lty = 4, col = 3)
lines(mu, power[,4], lty = 5, col = 4)
lines(mu, power[,5], lty = 1, col = 5)
legend("topright", c("sample size=10","sample size=20","sample size=30","sample size=40","sample size=50"), lty = c(2,3,4,5,1),col=c(1,2,3,4,5))
abline(v = 500, lty = 1)
lines(c(450,550),c(0.05,0.05))
```

Comment: when sample size increases, the power changes more abruptly. 

### 6.5
```{r,eval=TRUE, echo=TRUE, include=TRUE}
alpha <- 0.05
m <- 1000
n <- 20
qt <- qt(1-alpha/2, df = n-1)
LCL <- replicate(m, expr = {
  x <- rchisq(n, 2)
  return(mean(x) - qt * sd(x) / sqrt(n))
})
UCL <- replicate(m, expr = {
  x <- rchisq(n, 2)
  return(mean(x) + qt * sd(x) / sqrt(n))
})
mean((LCL < 2) * (UCL > 2))
```
My t-interval results is larger than the simulation results in Example 6.4, so it's more stable for departures from normality.

### 6.8
```{r,eval=TRUE, echo=TRUE, include=TRUE}
alpha = 0.055
mu1 <- mu2 <- 0
sigma1 <- 1
sigma2 <- 1.5
sample_num <- c(15, 100, 1000)

m <- 1000
tests_F <- numeric(3)
tests_CF <- numeric(3)

testF <- function(x, y) {
  f_test <- var.test(x, y, alternative = "two.sided", conf.level = 1-alpha)
  return(f_test$p.value < alpha)
}
tests5 = function(X, Y) {
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(as.integer(max(c(outx, outy)) > 5))
}
for (i in 1 : 3) {
  n <- sample_num[i]
  power_F <- mean(replicate(m, expr = {
  x <- rnorm(n, mu1, sigma1)
  y <- rnorm(n, mu2, sigma2)
  testF(x, y)
}))
  power_CF <- mean(replicate(m, expr = {
  x <- rnorm(n, mu1, sigma1)
  y <- rnorm(n, mu2, sigma2)
  tests5(x, y)
}))
  
  tests_F[i] <- power_F
  tests_CF[i] <- power_CF
}

(table = rbind(tests_F, tests_CF))
```

